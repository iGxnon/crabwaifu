listen_addr = "0.0.0.0:8808"
network = "both"

# Default llama model configuration, some of them can be overridden by client request
[llama]
# Available models:
# - https://huggingface.co/shenzhi-wang/Llama3.1-8B-Chinese-Chat/blob/main/gguf/llama3.1_8b_chinese_chat_q8_0.gguf
# - https://huggingface.co/lmstudio-ai/gemma-2b-it-GGUF/blob/main/gemma-2b-it-q8_0.gguf
# - ... llama family with Q8_0/Q4_0/Q4_1 quantization methods
model = "./TinyLLama-v0-5M-F16.gguf"
steps = 1024                         # The number of steps to run the model
probability = 0.9                    # The probability of the model to generate the next token, it is used to control the randomness of the model
temperature = 1.0                    # The temperature of the model, it is used to control the randomness of the model
threads = 8                          # Number of threads to run the model
mlock = false                        # Enable mlock to lock the memory, it reduces the risk of swapping to disk
f16_kv_cache = true                  # Enable f16 key-value cache, it reduces the memory usage
max_context_length = 4096            # It should not be less than the value in the model parameter file.

[whisper]
model = "./whisper-tiny-ggml-tiny.bin"  # The path to the whisper model
language = "en"                         # The language of the whisper model

[raknet]
sever_guid = 4170652262067404866
advertisement = "Hi, I am server"
min_mtu = 480
max_mtu = 1480
support_version = [9, 11, 13]
max_pending = 1024
max_parted_size = 256
max_parted_count = 256
max_channels = 255                # DO NOT MODIFY IT

[tcp]
ttl = 60
nodelay = false
