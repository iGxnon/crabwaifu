listen_addr = "0.0.0.0:8808"
network = "raknet"

# Default llama model configuration, some of them can be overriden by client request
[llama]
# Avaliable models:
# - https://huggingface.co/shenzhi-wang/Llama3.1-8B-Chinese-Chat/blob/main/gguf/llama3.1_8b_chinese_chat_q8_0.gguf
# - https://huggingface.co/lmstudio-ai/gemma-2b-it-GGUF/blob/main/gemma-2b-it-q8_0.gguf
# - ... llama family with Q8_0/Q4_0/Q4_1 quantization methods
model = "./TinyLLama-v0-5M-F16.gguf"
steps = 300
probability = 0.9
temperature = 1.0
threads = 8
mlock = false

[raknet]
send_buf_cap = 1024
sever_guid = 4170652262067404866
advertisement = "Hi, I am server"
min_mtu = 510
max_mtu = 1500
support_version = [9, 11, 13]
max_pending = 1024
max_parted_size = 256
max_parted_count = 256
max_channels = 64

[tcp]
ttl = 60
nodelay = true
